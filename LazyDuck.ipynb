{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os import walk\n",
    "import re\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks, find_peaks_cwt\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('default')\n",
    "\n",
    "from _Methods.Methods import * # collection of processing methods in _Methods directory\n",
    "\n",
    "data_dir = 'Messungen'\n",
    "\n",
    "dic = getFileContent(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Following files have been found:')\n",
    "for i in dic.keys():\n",
    "    print(dic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for cyclical analysis and ripping force analysis\n",
    "# read all csv data into one list \n",
    "datasets = {}#filename = []\n",
    "\n",
    "#renaming those absolutely horribly chosen column names\n",
    "names = ['Points','Elapsed_Time_Sec','Scan_Time_Sec','Disp_mm','Load_2_N','E12_%','E22_%','Axial_cmd_V','Axial_err_%','Unnamed']\n",
    "\n",
    "for key in dic.keys():\n",
    "    #print(key)\n",
    "    len_tmp = pd.read_csv(dic[key], index_col = False, names=names).shape[0]\n",
    "    if len_tmp >2:\n",
    "        datasets[key] = pd.read_csv(dic[key],header = 2, index_col = False, names=names)\n",
    "        datasets[key]['Short'] = 0\n",
    "        #if re.search(r\"reis\", key) == 1:\n",
    "        #    print('RISS')\n",
    "        #print(datasets[key].head())\n",
    "    else:\n",
    "        #print('tiny data set')\n",
    "        datasets[key] = pd.read_csv(dic[key], index_col = False, names=names)\n",
    "        datasets[key]['Short'] = 1\n",
    "        #print('short')\n",
    "    len_tmp = 0\n",
    "    \n",
    "\n",
    "ncyc = 4000\n",
    "n_init_cyc = {'20_10_1': ncyc,\n",
    "\t\t\t  '20_10_2': ncyc,\n",
    "\t\t\t  '20_10_3': ncyc,\n",
    "\t\t\t  '20_20_1': ncyc,\n",
    "\t\t\t  '20_20_2': ncyc,\n",
    "\t\t\t  '20_20_3': ncyc,\n",
    "\t\t\t  '40_1': ncyc,\n",
    "\t\t\t  '40_2': ncyc,\n",
    "\t\t\t  '40_3': ncyc}\n",
    "\n",
    " \n",
    "n_len_cyc  = 8000\n",
    "\n",
    "data_rip = {#'20_10_1_r': datasets['20190828 v20,10-1-reissen.CSV'],\n",
    "            #'20_10_2_r': datasets['20190828 v20,10-2-reissen.CSV'],\n",
    "            '20_10_3_r': datasets['20190828 v20-3-reissen.CSV'],\n",
    "            #'20_20_1_r': datasets['20190828 v20,20-1-reissen.CSV'],\n",
    "            '20_20_2_r': datasets['20190828 v20,20-2,reissen.CSV'],\n",
    "            #'20_20_3_r': datasets['20190828 v20,20-3,reissen.CSV'],\n",
    "            #'40_1_r': datasets['20190828 v40-1-reissen.CSV'],\n",
    "            '40_2_r': datasets['20190828 v40-2-reissen.CSV'],\n",
    "            #'40_3_r': datasets['20190828 v40-3-reissen.CSV'],\n",
    "            #'40_3_r2': datasets['20190828 v40-3-reissen2.CSV']\n",
    "            }\n",
    "\n",
    "data_cyc = {'20_10_1': datasets['20190828 v20,10-1.CSV'],\n",
    "            '20_10_2': datasets['20190828 v20,10-2.CSV'],\n",
    "            '20_10_3': datasets['20190828 v20-3.CSV'],\n",
    "            '20_20_1': datasets['20190828 v20,20-1.CSV'],\n",
    "            '20_20_2': datasets['20190828 v20,20-2.CSV'],\n",
    "            '20_20_3': datasets['20190828 v20,20-3.CSV'],\n",
    "            '40_1': datasets['20190828 v40-1.CSV'],\n",
    "            '40_2': datasets['20190828 v40-2.CSV'],\n",
    "            '40_3': datasets['20190828 v40-3.CSV'],\n",
    "            }\n",
    "\n",
    "#dimensions in the form height x width x length\n",
    "dimensions_mm = {'20_10_1':[0.580,5,2],\n",
    "                '20_10_2':[0.580,5,2],\n",
    "                '20_10_3':[0.580,5,1.5],\n",
    "                '20_20_1':[0.580,4.5,1.5],\n",
    "                '20_20_2':[0.580,5,1.5],\n",
    "                '20_20_3':[0.580,5,2],\n",
    "                '40_1':[0.580,4,1],\n",
    "                '40_2':[0.580,5,1],\n",
    "                '40_3':[0.580,3.5,1],\n",
    "                '20_10_1_r':[0.580,5,2],\n",
    "                '20_10_2_r':[0.580,5,2],\n",
    "                '20_10_3_r':[0.580,5,1.5],\n",
    "                '20_20_1_r':[0.580,4.5,1.5],\n",
    "                '20_20_2_r':[0.580,5,1.5],\n",
    "                '20_20_3_r':[0.580,5,2],\n",
    "                '40_1_r':[0.580,4,1],\n",
    "                '40_2_r':[0.580,5,1],\n",
    "                '40_3_r':[0.580,3.5,1],\n",
    "                '40_3_r2':[0.580,3.5,1]\n",
    "                }\n",
    "\n",
    "idx_max = {     #'20_10_1_r':0,\n",
    "                #'20_10_2_r':[1950,1970],\n",
    "                '20_10_3_r':[1950,2000],\n",
    "                #'20_20_1_r':0,\n",
    "                '20_20_2_r':[1485,1600],\n",
    "                #'20_20_3_r':0,\n",
    "                #'40_1_r':0,\n",
    "                '40_2_r':[1385,1500],\n",
    "                #'40_3_r':[1400,1415],\n",
    "                #'40_3_r2':[1180,1190]\n",
    "                }\n",
    "\n",
    "titles = {'20_10_1': 'ULD-V20-ULD, 10 % \\n Sample 1',\n",
    "            '20_10_2': 'Sample 2',\n",
    "            '20_10_3': 'Sample 3',\n",
    "            '20_20_1': 'ULD-V20-ULD, 20 % \\n Sample 1',\n",
    "            '20_20_2': 'Sample 2',\n",
    "            '20_20_3': 'Sample 3',\n",
    "            '40_1': 'ULD-V40-ULD, 20 % \\n Sample 1',\n",
    "            '40_2': 'Sample 2',\n",
    "            '40_3': 'Sample 3',\n",
    "            }\n",
    "\n",
    "\n",
    "# throw out all data before n_init\n",
    "for key in data_cyc.keys():\n",
    "    data_cyc[key] = data_cyc[key][data_cyc[key].Points != 1600]\n",
    "    data_cyc[key] = data_cyc[key][n_init_cyc[key]:n_init_cyc[key]+n_len_cyc].reset_index()\n",
    "\n",
    "data_rip['20_10_3_r'] = data_rip['20_10_3_r'].iloc[:-20] \n",
    "\n",
    "\n",
    "print(30*'='+'MAXIMUM STRESS'+30*'=')\n",
    "for key in data_rip.keys():\n",
    "    print('{} has shape '.format(key), data_rip[key].shape)\n",
    "\n",
    "print(70*'=')\n",
    "print(' ')\n",
    "\n",
    "print(30*'='+'CYCLICAL STRESS'+30*'=')\n",
    "for key in data_cyc.keys():\n",
    "    print('{} has shape '.format(key), data_cyc[key].shape)\n",
    "  #  print(data_cyc[key][795:815])\n",
    "print(70*'=')\n",
    "print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in data_rip.keys(): \n",
    "    data_rip[key]['Fs'] = savgol_filter(data_rip[key].Load_2_N,15,1)\n",
    "\n",
    "    #rename and drop columns\n",
    "    data_rip[key] = data_rip[key][['Disp_mm','Load_2_N','Fs']]\n",
    "    data_rip[key].columns = ['D','F','Fs']\n",
    "    #f = plt.figure(figsize = (14,12))\n",
    "    #ax = f.add_subplot()\n",
    "    #data_rip[key].plot(y=['Fs'], ax = ax)\n",
    "    #plt.plot(data_rip[key].D, s)\n",
    "    #####spit out plot\n",
    "    #data_rip[key].plot(y=['D','F','Fs'],figsize = (20,10), title = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dicts to store the standard deviations for rising and falling flanks for the different cyclical datasets    \n",
    "stds_rising = {}\n",
    "stds_falling = {}\n",
    "\n",
    "for key in data_cyc.keys():\n",
    "    \n",
    "    #create a mask for the fft and apply it. after that, shift data upwards to be above zero for causality\n",
    "    mask = np.ones((data_cyc[key].shape[0]))\n",
    "    \n",
    "    '''\n",
    "    t = np.abs(np.real(np.fft.fft(data_cyc[key].Load_2_N)))[:1000]\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.plot(t)\n",
    "    plt.show()\n",
    "    print(np.argsort(t))\n",
    "    '''\n",
    "    mask[0:200] = 0\n",
    "    mask[-199:] = 0\n",
    "    mask[201:-200] = 0\n",
    "\n",
    "    data_cyc[key]['Ff'] = filter_fourier(data_cyc[key].Load_2_N, mask)\n",
    "    data_cyc[key]['Ff'] = data_cyc[key]['Ff'] - np.min(data_cyc[key]['Ff'])\n",
    "    \n",
    "    #create rising and falling flags for all cyclical datasets\n",
    "    r_tmp = []\n",
    "    for i in range(data_cyc[key].shape[0]-1):\n",
    "        if data_cyc[key].Disp_mm.iloc[i] <= data_cyc[key].Disp_mm.iloc[i+1]:\n",
    "            r_tmp.append(1)\n",
    "        elif data_cyc[key].Disp_mm.iloc[i]> data_cyc[key].Disp_mm.iloc[i+1]:\n",
    "            r_tmp.append(-1)\n",
    "        if i == data_cyc[key].shape[0]-2:\n",
    "            r_tmp.append(r_tmp[-1])\n",
    " \n",
    "    data_cyc[key]['Rise'] = r_tmp\n",
    "    stds_rising[key] = np.var(data_cyc[key]['Ff'][data_cyc[key].Rise == 1])**0.5\n",
    "    stds_falling[key] = np.var(data_cyc[key]['Ff'][data_cyc[key].Rise == -1])**0.5\n",
    "    \n",
    "    n_bins = 13\n",
    "    #bin the displacement data for later use\n",
    "    data_cyc[key]['D_bin']  = pd.cut(data_cyc[key]['Disp_mm'],n_bins,labels=False)\n",
    "\n",
    "    #rename and drop columns\n",
    "    data_cyc[key] = data_cyc[key][['Disp_mm','Load_2_N','Ff','Rise','D_bin']]\n",
    "    data_cyc[key].columns = ['D','F','Ff','R','Dbin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#lets see what we can do here testwise\n",
    "sigmas = []\n",
    "epsilons = []\n",
    "ruptures = {}\n",
    "%matplotlib inline\n",
    "for key in data_rip.keys():\n",
    "    key = key\n",
    "    d = data_rip[key].copy()\n",
    " \n",
    "    id_max = idx_max[key][0]\n",
    "    id_min = idx_max[key][1]\n",
    "\n",
    "    h_gel_mm = dimensions_mm[key][0] \n",
    "    b_gel_mm = dimensions_mm[key][1]\n",
    "    l_gel_mm = dimensions_mm[key][2]\n",
    "\n",
    "    sigmas.append((d.Fs[id_max])/(b_gel_mm*h_gel_mm))\n",
    "    epsilons.append(d.D[id_max]/l_gel_mm)\n",
    "    data = []\n",
    "    dF = d.Fs[id_max:id_min].max() - d.Fs[id_max:id_min].min()\n",
    "    data = [1e6*(dF/(b_gel_mm*h_gel_mm)), (d.D[id_max]/l_gel_mm)]\n",
    "    ruptures[key] = data\n",
    "    \n",
    "    #f = plt.figure(figsize=(20,15))\n",
    "    #ax = f.add_subplot()\n",
    "    d.D  = d.D / l_gel_mm\n",
    "    #d.iloc[300:].plot(y=['F','Fs'],x='D', ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruptures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving rupture data to csv file\n",
    "#dfrupt = pd.DataFrame.from_dict(ruptures,orient='index').T\n",
    "#dfrupt.index = ['Stress at rupture in N/m^2','strain at rupture']\n",
    "#dfrupt\n",
    "#dfrupt.to_csv('rupture_data.csv')\n",
    "#ylims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['lines.linewidth'] = 1\n",
    "mpl.rcParams['lines.markersize'] = 2\n",
    "mpl.rcParams['legend.fontsize'] = 'small'\n",
    "mpl.rcParams['figure.titlesize'] = 'small'\n",
    "\n",
    "sigma_string = r'$\\sigma$  /  $\\left[\\dfrac{N}{m^2}\\right]$'\n",
    "eplsilon_string = r'$\\varepsilon$  /  $\\left[ \\% \\right]$'\n",
    "y_modulus = {}\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12,12))\n",
    "spec = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "nc = 0\n",
    "nr = 0\n",
    "\n",
    "\n",
    "# Set scale_axes = True, if plot axes shoudl be scaled to same maximum value fgür each gel type\n",
    "scale_axes = True\n",
    "ylims = {'0':30000,'1':35000,'2':52000}\n",
    "xlims = 22\n",
    "\n",
    "\n",
    "\n",
    "#iterate through columns\n",
    "for key in data_cyc.keys():\n",
    "    if nr == 3:\n",
    "        nc +=1\n",
    "        nr = 0\n",
    " \n",
    "    ax = fig.add_subplot(spec[nr,nc])\n",
    "    d = data_cyc[key]\n",
    "    \n",
    "    # get geometric data of gels for sigma and epsilon\n",
    "    \n",
    "    h_gel_mm = dimensions_mm[key][0] \n",
    "    b_gel_mm = dimensions_mm[key][1]\n",
    "    l_gel_mm = dimensions_mm[key][2]\n",
    "    \n",
    "    #plot 4 curves, each consisting of 2000 points, to investigate fatigue\n",
    "    \n",
    "    for k in range(1,5):\n",
    "       \n",
    "        dtot = data_cyc[key].iloc[(k-1)*2000:k*2000].copy()\n",
    "        mask = np.ones((dtot.shape[0]))\n",
    "        mask[0:50] = 0\n",
    "        mask[-49:] = 0\n",
    "\n",
    "        mask[51:-50] = 0\n",
    "\n",
    "        dtot['Ff'] = filter_fourier(dtot.F, mask)\n",
    "        dtot['Ff'] = dtot['Ff'] - np.min(dtot['Ff'])\n",
    "        dtot['Eps'] = 100 * (dtot.D / l_gel_mm)\n",
    "        dtot['Sigma'] = 1e6 * (dtot.Ff / (h_gel_mm*b_gel_mm))\n",
    "        \n",
    "        dtot.plot(x='Eps',y='Sigma', ax = ax, color = (.7**k, .7**k, .95**k), label = 'cycles '  + str(int((k-1)*2000/40+1))+ '-' +str(int(k*2000/40)))\n",
    "                \n",
    "        df = np.max(dtot['Ff']) - np.min(dtot['Ff'])\n",
    "        dsigma = df/(b_gel_mm*h_gel_mm)\n",
    "        \n",
    "        epsilon = (np.max(dtot['D']) - np.min(dtot['D']))/l_gel_mm\n",
    "        #save the youngs modulus for each cycle group\n",
    "        y_modulus[key+'_'+str(k)] = 1e6*dsigma/epsilon\n",
    "        \n",
    "        # remove legends from all subplots\n",
    "        ax.get_legend().remove()\n",
    "        \n",
    "   # ax.set_ylim([-0.01,0.10])\n",
    "    ax.set_title(titles[key])\n",
    "    ax.set_xlabel(eplsilon_string)\n",
    "    ax.set_ylabel(sigma_string)\n",
    "    \n",
    "    if scale_axes:\n",
    "        ax.set_ylim([-0.1*ylims[str(nc)],ylims[str(nc)]])\n",
    "        ax.set_xlim([-0.1*xlims,xlims])\n",
    "        ax.grid(True)\n",
    "    nr += 1\n",
    "    \n",
    "#get handels and labels for the last axes to create a single legend above all subplots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# copy the handles\n",
    "\n",
    "handles = [copy.copy(ha) for ha in handles]\n",
    "\n",
    "# set the linewidths to the copies\n",
    "[ha.set_linewidth(7) for ha in handles]\n",
    "\n",
    "fig.legend(handles, \n",
    "           labels, \n",
    "           ncol=4, \n",
    "           borderaxespad=0., \n",
    "           fontsize = 14,\n",
    "           mode=\"expand\",\n",
    "           #loc='upper center', \n",
    "           bbox_to_anchor=(0., .95, 1., .102))\n",
    "\n",
    "\n",
    "#fig.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "#plt.tight_layout()\n",
    "    \n",
    "nc = 0    \n",
    "\n",
    "fig.savefig('Figure3_DMA.svg',format='svg',bbox_inches = 'tight')\n",
    "fig.savefig('Figure3_DMA.png',format='png',bbox_inches = 'tight',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_modulus\n",
    "dfyoung = pd.DataFrame.from_dict(y_modulus,orient='index').reset_index()\n",
    "dfyoung.columns=['Gel','E']\n",
    "dfyoung.Gel = dfyoung.Gel.str.slice(0,-2)\n",
    "dfyoung_grouped = dfyoung.groupby('Gel').agg({'E': [np.mean,np.std]}).round(2)\n",
    "dfyoung_grouped.columns = ['E_mu','E_std']\n",
    "dfyoung_grouped.to_csv('y_mod_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (10,10))\n",
    "ax = f.add_subplot(1,1,1)\n",
    "data_cyc['20_10_1'][:500].plot(y='Ff', ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.DataFrame.from_dict(y_modulus,orient='index')\n",
    "#sub\n",
    "#sub.to_csv('y_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ruptures\n",
    "#sub_rup = pd.DataFrame.from_dict(ruptures, orient='index')\n",
    "#sub_rup.to_csv('rup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##=========================================================##\n",
    "##HERE COMES THE DMA DATA FROM THE 22.12.2020##\n",
    "data_dir = 'Messungen/DMA 22.12.2020/'\n",
    "files = getFileContent(data_dir)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "#renaming those absolutely horribly chosen column names\n",
    "names = ['Points','Elapsed_Time_Sec','Scan_Time_Sec','Disp_mm','Load_2_N','E12_%','E22_%','Axial_cmd_V','Axial_err_%','Unnamed']\n",
    "k=0\n",
    "for i in files.keys():\n",
    "    df = pd.read_csv(files[i],header = 2, index_col = False,names=names)\n",
    "    if (df.shape[0]<10000) | (files[i][-5]  != '1'):\n",
    "        continue\n",
    "    df = df.iloc[4000:12000].Load_2_N.values\n",
    "    print(i)\n",
    "    \n",
    "    #obtain hightes spectral energy but kill dc component at index zero\n",
    "    spectrum = df.copy()\n",
    "    spectrum = np.fft.fft(spectrum)#.reshape(8000,1)\n",
    "    spectrum[0] = 0\n",
    "    figsize = (14,12)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(spectrum)\n",
    "    plt.title(i)\n",
    "    \n",
    "    print(np.argmax(spectrum[:4000]))\n",
    "    f_accept = np.argmax(np.abs(np.real(spectrum[:4000])))\n",
    "    bw = 0\n",
    "    \n",
    "    mask = np.zeros((8000))\n",
    "    mask[f_accept-bw:f_accept+bw+1] = 1\n",
    "    mask[-f_accept-bw:-f_accept+bw+1] = 1\n",
    "    \n",
    "    result = spectrum*mask\n",
    "    result = np.real(np.fft.ifft(result))\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(result[:1000],alpha=0.45, label=i)\n",
    "    plt.plot(df[:1000],alpha=0.45)\n",
    "    plt.legend()\n",
    "    k+=1\n",
    "    if k==20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =\n",
    "b = \n",
    "c = \n",
    "\n",
    "anregungsfrequenz =\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
